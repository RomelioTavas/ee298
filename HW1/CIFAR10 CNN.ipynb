{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp, Dataset Augmentation, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 1.4359 - acc: 0.5206 - val_loss: 1.1409 - val_acc: 0.6144\n",
      "Epoch 2/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.9614 - acc: 0.6686 - val_loss: 1.1649 - val_acc: 0.6063\n",
      "Epoch 3/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.8323 - acc: 0.7161 - val_loss: 0.8209 - val_acc: 0.7254\n",
      "Epoch 4/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.7487 - acc: 0.7449 - val_loss: 0.7465 - val_acc: 0.7482\n",
      "Epoch 5/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.6876 - acc: 0.7670 - val_loss: 0.7725 - val_acc: 0.7441\n",
      "Epoch 6/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.6401 - acc: 0.7824 - val_loss: 0.6752 - val_acc: 0.7693\n",
      "Epoch 7/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.6037 - acc: 0.7955 - val_loss: 0.7806 - val_acc: 0.7423\n",
      "Epoch 8/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.5708 - acc: 0.8083 - val_loss: 0.6863 - val_acc: 0.7710\n",
      "Epoch 9/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.5414 - acc: 0.8172 - val_loss: 0.7034 - val_acc: 0.7666\n",
      "Epoch 10/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.5249 - acc: 0.8230 - val_loss: 0.6997 - val_acc: 0.7655\n",
      "Epoch 11/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.5005 - acc: 0.8319 - val_loss: 0.6053 - val_acc: 0.7987\n",
      "Epoch 12/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.4777 - acc: 0.8380 - val_loss: 0.6577 - val_acc: 0.7849\n",
      "Epoch 13/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.4628 - acc: 0.8439 - val_loss: 0.5967 - val_acc: 0.8042\n",
      "Epoch 14/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.4500 - acc: 0.8478 - val_loss: 0.6209 - val_acc: 0.7930\n",
      "Epoch 15/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.4330 - acc: 0.8543 - val_loss: 0.6634 - val_acc: 0.7829\n",
      "Epoch 16/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.4207 - acc: 0.8576 - val_loss: 0.5741 - val_acc: 0.8071\n",
      "Epoch 17/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.4088 - acc: 0.8621 - val_loss: 0.5709 - val_acc: 0.8127\n",
      "Epoch 18/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.4018 - acc: 0.8642 - val_loss: 0.5829 - val_acc: 0.8113\n",
      "Epoch 19/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.3860 - acc: 0.8689 - val_loss: 0.5670 - val_acc: 0.8158\n",
      "Epoch 20/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.3734 - acc: 0.8731 - val_loss: 0.5854 - val_acc: 0.8038\n",
      "Epoch 21/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.3687 - acc: 0.8741 - val_loss: 0.7552 - val_acc: 0.7517\n",
      "Epoch 22/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.3584 - acc: 0.8790 - val_loss: 0.5917 - val_acc: 0.8017\n",
      "Epoch 23/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.3525 - acc: 0.8801 - val_loss: 0.6233 - val_acc: 0.7966\n",
      "Epoch 24/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.3493 - acc: 0.8810 - val_loss: 0.7206 - val_acc: 0.7713\n",
      "Epoch 25/200\n",
      "1563/1562 [==============================] - 16s 11ms/step - loss: 0.2777 - acc: 0.9057 - val_loss: 0.4930 - val_acc: 0.8433\n",
      "Epoch 26/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.2669 - acc: 0.9106 - val_loss: 0.4992 - val_acc: 0.8427\n",
      "Epoch 27/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.2589 - acc: 0.9122 - val_loss: 0.5198 - val_acc: 0.8363\n",
      "Epoch 28/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.2508 - acc: 0.9141 - val_loss: 0.4952 - val_acc: 0.8419\n",
      "Epoch 29/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.2503 - acc: 0.9142 - val_loss: 0.4775 - val_acc: 0.8453\n",
      "Epoch 30/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.2440 - acc: 0.9166 - val_loss: 0.5044 - val_acc: 0.8355\n",
      "Epoch 31/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.2378 - acc: 0.9182 - val_loss: 0.5001 - val_acc: 0.8423\n",
      "Epoch 32/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.2365 - acc: 0.9206 - val_loss: 0.4853 - val_acc: 0.8464\n",
      "Epoch 33/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.2336 - acc: 0.9216 - val_loss: 0.4767 - val_acc: 0.8460\n",
      "Epoch 34/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.2309 - acc: 0.9206 - val_loss: 0.4822 - val_acc: 0.8451\n",
      "Epoch 35/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.2290 - acc: 0.9218 - val_loss: 0.5042 - val_acc: 0.8418\n",
      "Epoch 36/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.2255 - acc: 0.9208 - val_loss: 0.4793 - val_acc: 0.8450\n",
      "Epoch 37/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.2244 - acc: 0.9221 - val_loss: 0.4926 - val_acc: 0.8463\n",
      "Epoch 38/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.2168 - acc: 0.9257 - val_loss: 0.5385 - val_acc: 0.8401\n",
      "Epoch 39/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1983 - acc: 0.9316 - val_loss: 0.4746 - val_acc: 0.8534\n",
      "Epoch 40/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1979 - acc: 0.9320 - val_loss: 0.4657 - val_acc: 0.8537\n",
      "Epoch 41/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1945 - acc: 0.9335 - val_loss: 0.4817 - val_acc: 0.8527\n",
      "Epoch 42/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1862 - acc: 0.9354 - val_loss: 0.4801 - val_acc: 0.8525\n",
      "Epoch 43/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1877 - acc: 0.9354 - val_loss: 0.4736 - val_acc: 0.8530\n",
      "Epoch 44/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1906 - acc: 0.9342 - val_loss: 0.4730 - val_acc: 0.8528\n",
      "Epoch 45/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1861 - acc: 0.9357 - val_loss: 0.4708 - val_acc: 0.8552\n",
      "Epoch 46/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1809 - acc: 0.9387 - val_loss: 0.4713 - val_acc: 0.8543\n",
      "Epoch 47/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1781 - acc: 0.9390 - val_loss: 0.4735 - val_acc: 0.8552\n",
      "Epoch 48/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1800 - acc: 0.9387 - val_loss: 0.4811 - val_acc: 0.8543\n",
      "Epoch 49/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1791 - acc: 0.9392 - val_loss: 0.4746 - val_acc: 0.8555\n",
      "Epoch 50/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1735 - acc: 0.9410 - val_loss: 0.4718 - val_acc: 0.8537\n",
      "Epoch 51/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1759 - acc: 0.9395 - val_loss: 0.4733 - val_acc: 0.8562\n",
      "Epoch 52/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1759 - acc: 0.9399 - val_loss: 0.4746 - val_acc: 0.8553\n",
      "Epoch 53/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1757 - acc: 0.9403 - val_loss: 0.4734 - val_acc: 0.8544\n",
      "Epoch 54/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1740 - acc: 0.9408 - val_loss: 0.4760 - val_acc: 0.8555\n",
      "Epoch 55/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1761 - acc: 0.9404 - val_loss: 0.4736 - val_acc: 0.8558\n",
      "Epoch 56/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1743 - acc: 0.9396 - val_loss: 0.4734 - val_acc: 0.8560\n",
      "Epoch 57/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1750 - acc: 0.9411 - val_loss: 0.4737 - val_acc: 0.8559\n",
      "Epoch 58/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1771 - acc: 0.9392 - val_loss: 0.4737 - val_acc: 0.8559\n",
      "Epoch 59/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1747 - acc: 0.9407 - val_loss: 0.4712 - val_acc: 0.8560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1778 - acc: 0.9391 - val_loss: 0.4730 - val_acc: 0.8560\n",
      "Epoch 61/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1757 - acc: 0.9401 - val_loss: 0.4728 - val_acc: 0.8558\n",
      "Epoch 62/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1743 - acc: 0.9398 - val_loss: 0.4737 - val_acc: 0.8559\n",
      "Epoch 63/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1725 - acc: 0.9409 - val_loss: 0.4731 - val_acc: 0.8558\n",
      "Epoch 64/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1743 - acc: 0.9392 - val_loss: 0.4713 - val_acc: 0.8553\n",
      "Epoch 65/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1736 - acc: 0.9405 - val_loss: 0.4752 - val_acc: 0.8555\n",
      "Epoch 66/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1739 - acc: 0.9405 - val_loss: 0.4728 - val_acc: 0.8559\n",
      "Epoch 67/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1724 - acc: 0.9412 - val_loss: 0.4744 - val_acc: 0.8563\n",
      "Epoch 68/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1755 - acc: 0.9404 - val_loss: 0.4715 - val_acc: 0.8555\n",
      "Epoch 69/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1746 - acc: 0.9405 - val_loss: 0.4722 - val_acc: 0.8561\n",
      "Epoch 70/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1752 - acc: 0.9407 - val_loss: 0.4720 - val_acc: 0.8559\n",
      "Epoch 71/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1769 - acc: 0.9385 - val_loss: 0.4733 - val_acc: 0.8557\n",
      "Epoch 72/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1757 - acc: 0.9391 - val_loss: 0.4726 - val_acc: 0.8556\n",
      "Epoch 73/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.1746 - acc: 0.9399 - val_loss: 0.4728 - val_acc: 0.8552\n",
      "Epoch 74/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1774 - acc: 0.9390 - val_loss: 0.4757 - val_acc: 0.8553\n",
      "Epoch 75/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1746 - acc: 0.9397 - val_loss: 0.4738 - val_acc: 0.8557\n",
      "Epoch 76/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1762 - acc: 0.9395 - val_loss: 0.4740 - val_acc: 0.8555\n",
      "Epoch 77/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1757 - acc: 0.9403 - val_loss: 0.4717 - val_acc: 0.8554\n",
      "Epoch 78/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1731 - acc: 0.9393 - val_loss: 0.4724 - val_acc: 0.8557\n",
      "Epoch 79/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1715 - acc: 0.9404 - val_loss: 0.4716 - val_acc: 0.8555\n",
      "Epoch 80/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1742 - acc: 0.9401 - val_loss: 0.4735 - val_acc: 0.8553\n",
      "Epoch 81/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1731 - acc: 0.9402 - val_loss: 0.4738 - val_acc: 0.8552\n",
      "Epoch 82/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1721 - acc: 0.9416 - val_loss: 0.4747 - val_acc: 0.8555\n",
      "Epoch 83/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1744 - acc: 0.9408 - val_loss: 0.4734 - val_acc: 0.8559\n",
      "Epoch 84/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1738 - acc: 0.9401 - val_loss: 0.4744 - val_acc: 0.8554\n",
      "Epoch 85/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1721 - acc: 0.9416 - val_loss: 0.4728 - val_acc: 0.8555\n",
      "Epoch 86/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1730 - acc: 0.9406 - val_loss: 0.4714 - val_acc: 0.8552\n",
      "Epoch 87/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1721 - acc: 0.9416 - val_loss: 0.4732 - val_acc: 0.8554\n",
      "Epoch 88/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1726 - acc: 0.9416 - val_loss: 0.4726 - val_acc: 0.8553\n",
      "Epoch 89/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1739 - acc: 0.9398 - val_loss: 0.4723 - val_acc: 0.8551\n",
      "Epoch 90/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1727 - acc: 0.9410 - val_loss: 0.4731 - val_acc: 0.8551\n",
      "Epoch 91/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1727 - acc: 0.9412 - val_loss: 0.4737 - val_acc: 0.8557\n",
      "Epoch 92/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1732 - acc: 0.9421 - val_loss: 0.4729 - val_acc: 0.8556\n",
      "Epoch 93/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1744 - acc: 0.9394 - val_loss: 0.4742 - val_acc: 0.8549\n",
      "Epoch 94/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1746 - acc: 0.9410 - val_loss: 0.4742 - val_acc: 0.8563\n",
      "Epoch 95/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1738 - acc: 0.9401 - val_loss: 0.4730 - val_acc: 0.8556\n",
      "Epoch 96/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1749 - acc: 0.9399 - val_loss: 0.4742 - val_acc: 0.8553\n",
      "Epoch 97/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1735 - acc: 0.9400 - val_loss: 0.4742 - val_acc: 0.8557\n",
      "Epoch 98/200\n",
      "1563/1562 [==============================] - 19s 12ms/step - loss: 0.1759 - acc: 0.9393 - val_loss: 0.4728 - val_acc: 0.8555\n",
      "Epoch 99/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1711 - acc: 0.9415 - val_loss: 0.4722 - val_acc: 0.8551\n",
      "Epoch 100/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1730 - acc: 0.9409 - val_loss: 0.4740 - val_acc: 0.8558\n",
      "Epoch 101/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1717 - acc: 0.9416 - val_loss: 0.4731 - val_acc: 0.8558\n",
      "Epoch 102/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1728 - acc: 0.9411 - val_loss: 0.4722 - val_acc: 0.8557\n",
      "Epoch 103/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1741 - acc: 0.9406 - val_loss: 0.4746 - val_acc: 0.8559\n",
      "Epoch 104/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1739 - acc: 0.9407 - val_loss: 0.4735 - val_acc: 0.8555\n",
      "Epoch 105/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1732 - acc: 0.9399 - val_loss: 0.4728 - val_acc: 0.8558\n",
      "Epoch 106/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1729 - acc: 0.9410 - val_loss: 0.4714 - val_acc: 0.8553\n",
      "Epoch 107/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1739 - acc: 0.9398 - val_loss: 0.4729 - val_acc: 0.8557\n",
      "Epoch 108/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1706 - acc: 0.9420 - val_loss: 0.4732 - val_acc: 0.8554\n",
      "Epoch 109/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1737 - acc: 0.9395 - val_loss: 0.4731 - val_acc: 0.8555\n",
      "Epoch 110/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1743 - acc: 0.9397 - val_loss: 0.4716 - val_acc: 0.8555\n",
      "Epoch 111/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1728 - acc: 0.9405 - val_loss: 0.4725 - val_acc: 0.8551\n",
      "Epoch 112/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1755 - acc: 0.9396 - val_loss: 0.4727 - val_acc: 0.8554\n",
      "Epoch 113/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1747 - acc: 0.9396 - val_loss: 0.4728 - val_acc: 0.8555\n",
      "Epoch 114/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1742 - acc: 0.9404 - val_loss: 0.4728 - val_acc: 0.8554\n",
      "Epoch 115/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1732 - acc: 0.9408 - val_loss: 0.4746 - val_acc: 0.8556\n",
      "Epoch 116/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1764 - acc: 0.9383 - val_loss: 0.4740 - val_acc: 0.8555\n",
      "Epoch 117/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1732 - acc: 0.9404 - val_loss: 0.4725 - val_acc: 0.8556\n",
      "Epoch 118/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1742 - acc: 0.9407 - val_loss: 0.4735 - val_acc: 0.8555\n",
      "Epoch 119/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1726 - acc: 0.9401 - val_loss: 0.4707 - val_acc: 0.8557\n",
      "Epoch 120/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1738 - acc: 0.9402 - val_loss: 0.4749 - val_acc: 0.8551\n",
      "Epoch 121/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1711 - acc: 0.9424 - val_loss: 0.4757 - val_acc: 0.8556\n",
      "Epoch 122/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1712 - acc: 0.9425 - val_loss: 0.4740 - val_acc: 0.8553\n",
      "Epoch 123/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1725 - acc: 0.9410 - val_loss: 0.4727 - val_acc: 0.8556\n",
      "Epoch 124/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1756 - acc: 0.9395 - val_loss: 0.4727 - val_acc: 0.8558\n",
      "Epoch 125/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1726 - acc: 0.9411 - val_loss: 0.4730 - val_acc: 0.8553\n",
      "Epoch 126/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1747 - acc: 0.9387 - val_loss: 0.4728 - val_acc: 0.8554\n",
      "Epoch 127/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1736 - acc: 0.9402 - val_loss: 0.4722 - val_acc: 0.8555\n",
      "Epoch 128/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1727 - acc: 0.9404 - val_loss: 0.4720 - val_acc: 0.8552\n",
      "Epoch 129/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1721 - acc: 0.9417 - val_loss: 0.4742 - val_acc: 0.8554\n",
      "Epoch 130/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1731 - acc: 0.9406 - val_loss: 0.4729 - val_acc: 0.8554\n",
      "Epoch 131/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1712 - acc: 0.9405 - val_loss: 0.4746 - val_acc: 0.8551\n",
      "Epoch 132/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1743 - acc: 0.9404 - val_loss: 0.4726 - val_acc: 0.8550\n",
      "Epoch 133/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1732 - acc: 0.9408 - val_loss: 0.4727 - val_acc: 0.8550\n",
      "Epoch 134/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1740 - acc: 0.9401 - val_loss: 0.4749 - val_acc: 0.8545\n",
      "Epoch 135/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1715 - acc: 0.9407 - val_loss: 0.4715 - val_acc: 0.8554\n",
      "Epoch 136/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1705 - acc: 0.9407 - val_loss: 0.4736 - val_acc: 0.8557\n",
      "Epoch 137/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.1695 - acc: 0.9418 - val_loss: 0.4736 - val_acc: 0.8551\n",
      "Epoch 138/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.1745 - acc: 0.9407 - val_loss: 0.4760 - val_acc: 0.8551\n",
      "Epoch 139/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.1731 - acc: 0.9402 - val_loss: 0.4739 - val_acc: 0.8562\n",
      "Epoch 140/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.1753 - acc: 0.9403 - val_loss: 0.4748 - val_acc: 0.8553\n",
      "Epoch 141/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.1741 - acc: 0.9405 - val_loss: 0.4740 - val_acc: 0.8559\n",
      "Epoch 142/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1742 - acc: 0.9403 - val_loss: 0.4727 - val_acc: 0.8552\n",
      "Epoch 143/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1736 - acc: 0.9401 - val_loss: 0.4741 - val_acc: 0.8557\n",
      "Epoch 144/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1707 - acc: 0.9412 - val_loss: 0.4749 - val_acc: 0.8552\n",
      "Epoch 145/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1725 - acc: 0.9410 - val_loss: 0.4718 - val_acc: 0.8554\n",
      "Epoch 146/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1722 - acc: 0.9409 - val_loss: 0.4738 - val_acc: 0.8556\n",
      "Epoch 147/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1737 - acc: 0.9400 - val_loss: 0.4734 - val_acc: 0.8552\n",
      "Epoch 148/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1733 - acc: 0.9406 - val_loss: 0.4728 - val_acc: 0.8552\n",
      "Epoch 149/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1715 - acc: 0.9413 - val_loss: 0.4732 - val_acc: 0.8556\n",
      "Epoch 150/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1710 - acc: 0.9418 - val_loss: 0.4724 - val_acc: 0.8552\n",
      "Epoch 151/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1745 - acc: 0.9391 - val_loss: 0.4755 - val_acc: 0.8550\n",
      "Epoch 152/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1741 - acc: 0.9400 - val_loss: 0.4723 - val_acc: 0.8561\n",
      "Epoch 153/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1727 - acc: 0.9405 - val_loss: 0.4743 - val_acc: 0.8553\n",
      "Epoch 154/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1711 - acc: 0.9404 - val_loss: 0.4723 - val_acc: 0.8556\n",
      "Epoch 155/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1732 - acc: 0.9402 - val_loss: 0.4740 - val_acc: 0.8559\n",
      "Epoch 156/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1742 - acc: 0.9392 - val_loss: 0.4724 - val_acc: 0.8556\n",
      "Epoch 157/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1749 - acc: 0.9394 - val_loss: 0.4739 - val_acc: 0.8555\n",
      "Epoch 158/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1756 - acc: 0.9400 - val_loss: 0.4748 - val_acc: 0.8552\n",
      "Epoch 159/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1731 - acc: 0.9415 - val_loss: 0.4737 - val_acc: 0.8554\n",
      "Epoch 160/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.1721 - acc: 0.9417 - val_loss: 0.4736 - val_acc: 0.8554\n",
      "Epoch 161/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1708 - acc: 0.9419 - val_loss: 0.4730 - val_acc: 0.8554\n",
      "Epoch 162/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1735 - acc: 0.9400 - val_loss: 0.4735 - val_acc: 0.8547\n",
      "Epoch 163/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1741 - acc: 0.9401 - val_loss: 0.4726 - val_acc: 0.8556\n",
      "Epoch 164/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1706 - acc: 0.9413 - val_loss: 0.4723 - val_acc: 0.8559\n",
      "Epoch 165/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1713 - acc: 0.9413 - val_loss: 0.4727 - val_acc: 0.8557\n",
      "Epoch 166/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1720 - acc: 0.9414 - val_loss: 0.4737 - val_acc: 0.8558\n",
      "Epoch 167/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1690 - acc: 0.9412 - val_loss: 0.4742 - val_acc: 0.8551\n",
      "Epoch 168/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1717 - acc: 0.9414 - val_loss: 0.4736 - val_acc: 0.8554\n",
      "Epoch 169/200\n",
      "1563/1562 [==============================] - 16s 11ms/step - loss: 0.1746 - acc: 0.9406 - val_loss: 0.4725 - val_acc: 0.8559\n",
      "Epoch 170/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1749 - acc: 0.9397 - val_loss: 0.4731 - val_acc: 0.8555\n",
      "Epoch 171/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1709 - acc: 0.9410 - val_loss: 0.4719 - val_acc: 0.8561\n",
      "Epoch 172/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1740 - acc: 0.9403 - val_loss: 0.4732 - val_acc: 0.8554\n",
      "Epoch 173/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1725 - acc: 0.9412 - val_loss: 0.4740 - val_acc: 0.8555\n",
      "Epoch 174/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1703 - acc: 0.9411 - val_loss: 0.4734 - val_acc: 0.8553\n",
      "Epoch 175/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1727 - acc: 0.9419 - val_loss: 0.4754 - val_acc: 0.8553\n",
      "Epoch 176/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1730 - acc: 0.9411 - val_loss: 0.4736 - val_acc: 0.8561\n",
      "Epoch 177/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1757 - acc: 0.9396 - val_loss: 0.4717 - val_acc: 0.8555\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1728 - acc: 0.9409 - val_loss: 0.4753 - val_acc: 0.8547\n",
      "Epoch 179/200\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.1706 - acc: 0.9405 - val_loss: 0.4750 - val_acc: 0.8558\n",
      "Epoch 180/200\n",
      "1563/1562 [==============================] - 16s 11ms/step - loss: 0.1730 - acc: 0.9399 - val_loss: 0.4732 - val_acc: 0.8553\n",
      "Epoch 181/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1703 - acc: 0.9420 - val_loss: 0.4749 - val_acc: 0.8554\n",
      "Epoch 182/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1759 - acc: 0.9394 - val_loss: 0.4749 - val_acc: 0.8554\n",
      "Epoch 183/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1715 - acc: 0.9408 - val_loss: 0.4733 - val_acc: 0.8557\n",
      "Epoch 184/200\n",
      "1563/1562 [==============================] - 16s 11ms/step - loss: 0.1768 - acc: 0.9402 - val_loss: 0.4731 - val_acc: 0.8549\n",
      "Epoch 185/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1738 - acc: 0.9405 - val_loss: 0.4717 - val_acc: 0.8553\n",
      "Epoch 186/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1737 - acc: 0.9417 - val_loss: 0.4751 - val_acc: 0.8550\n",
      "Epoch 187/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1746 - acc: 0.9395 - val_loss: 0.4726 - val_acc: 0.8559\n",
      "Epoch 188/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1763 - acc: 0.9394 - val_loss: 0.4754 - val_acc: 0.8553\n",
      "Epoch 189/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1715 - acc: 0.9403 - val_loss: 0.4716 - val_acc: 0.8556\n",
      "Epoch 190/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1747 - acc: 0.9405 - val_loss: 0.4743 - val_acc: 0.8558\n",
      "Epoch 191/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1743 - acc: 0.9405 - val_loss: 0.4736 - val_acc: 0.8555\n",
      "Epoch 192/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1719 - acc: 0.9407 - val_loss: 0.4726 - val_acc: 0.8551\n",
      "Epoch 193/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1711 - acc: 0.9416 - val_loss: 0.4733 - val_acc: 0.8555\n",
      "Epoch 194/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1749 - acc: 0.9407 - val_loss: 0.4720 - val_acc: 0.8558\n",
      "Epoch 195/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1719 - acc: 0.9404 - val_loss: 0.4746 - val_acc: 0.8556\n",
      "Epoch 196/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1749 - acc: 0.9396 - val_loss: 0.4736 - val_acc: 0.8556\n",
      "Epoch 197/200\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.1711 - acc: 0.9410 - val_loss: 0.4735 - val_acc: 0.8555\n",
      "Epoch 198/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1731 - acc: 0.9402 - val_loss: 0.4742 - val_acc: 0.8558\n",
      "Epoch 199/200\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.1714 - acc: 0.9417 - val_loss: 0.4744 - val_acc: 0.8551\n",
      "Epoch 200/200\n",
      "1563/1562 [==============================] - 18s 12ms/step - loss: 0.1710 - acc: 0.9410 - val_loss: 0.4741 - val_acc: 0.8557\n",
      "Saved trained model at /home/samsung/Projects/Bong/EE298/HW1/saved_models/keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 2s 159us/step\n",
      "Test loss: 0.4741211565256119\n",
      "Test accuracy: 0.8557\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "# Training Params\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_cnn_trained_model.h5'\n",
    "initial_lr=0.0004\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=initial_lr, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Use dataset augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    shear_range=0.,  # set range for random shear\n",
    "    zoom_range=0.,  # set range for random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# Tensorboard Callback\n",
    "tbCallBack = keras.callbacks.TensorBoard(\n",
    "    batch_size=batch_size,\n",
    "    log_dir=\"logs/{}\".format(time()),\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq='batch'\n",
    ")\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [tbCallBack, lr_reducer]\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                    batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    workers=4,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
